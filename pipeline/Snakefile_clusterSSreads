from collections import defaultdict
import os
import subprocess

configfile: "config.yaml"
SHORTREADS = config["shortreads"]
ALIGNERBINPATH = config["graphalignerfolder"]
SCRIPTPATH = config["graphalignerfolder"] + "scripts"
BCALMPATH = config["bcalmpath"]
CONVERTTOGFAPATH = config["bcalm_converttoGFApath"]
BGREAT = config["bgreat"]
EXTRACTFASTQ = config["extract_fastq"]
inputDir = config["input_dir"]
softClustDir = config["soft_clust_dir"]
outputDir = config["output_dir"]
SSfastqDir = config["SS_fastq_dir"]
VG = config["VG"]
nodelens = config["node_lens"]
chroms = [str(x) for x in range(1, 23)] + ["X", "Y"]
exportFasta = config["export_fasta_path"]
computeSimpleBubble = config["simple_bubble_detection_path"]
getSNVbubbles = config["get_snv_bubbles_path"]
getRevcompFree = config["get_rev_comp_free_path"]
clustersizes = config["cluster_sizes"]

###############################################
#######		python functions	#######
###############################################

revcomp = {'a':'t', 'c':'g', 'g':'c', 't':'a', 'A':'T', 'C':'G', 'G':'C', 'T':'A'}

def reversecomp(seq):
	rc = ''
	for i in range(len(seq)):
		rc = revcomp[seq[i]] + rc
	return rc

def revcompfasta(infile, outfile):
	with open(infile) as f:
		with open(outfile, 'a') as out:
			for line in f:
				if line[0]==">":
					print(line.strip(), file=out)
				else:
					print(reversecomp(line.strip()), file=out)

def getclustersize(cl, filename):
	#print("awk \'$3==\"" + cl + "\" {print $2}\' " + "config[\"cluster_sizes\"]")
	return subprocess.getoutput("awk \'$3==\"" + cl + "\" {print $2}\' " + filename)

print("getclustersize(V23) = ", getclustersize('V23', clustersizes))
		

###############################################

# outout dir should have these two files: cluster_name_mapping.data and clust_partners.txt
# getting the names of first cluster pairs:
f = open(outputDir+"/clust_partners.txt")
clustpair = {}
sp = f.read().strip().split("\n")
print(sp)
for s in range(1, len(sp)):
	clust = sp[s].split("\t")
	clustpair[clust[1]]=clust[2]

print("clustpair = ", clustpair)
print([x + "_rc" + clustpair[x] for x in clustpair])

clusters = list(clustpair.keys())+list(clustpair.values())


wildcard_constraints:
	graphname = "k\d+_a\d+_u\d+",
	k = "\d+",
	a = "\d+",
	u = "\d+",
	longnodesize = "\d+",
	overlapsize = "\d+",
	longreads = "[^_]+",
	shortreads = "[^_]+",
	lib = "[^_]+_[0-9]+",
	clust = "[V]\d+",



sample, chunkID, = glob_wildcards(inputDir+"/{sample}_chunk{chunkID}.maf.gz")
sample=set(sample)
libs, L, = glob_wildcards(SSfastqDir + "/{lib}_L{L}_R1_001.fastq.gz")


rule all:
	input:
		expand(outputDir+"/splitted-per-cluster/Canu-assembly_{sample}_cluster{clust}_with_RCpair/PacBio_errorRate{error}_overlap{overlap}.contigs.fasta", 
			sample=sample, clust=clustpair.keys(), error=config["errorRate"], overlap=config["overlapLen"])
		

rule add_soft_clust_to_original_map_files:
	input:
		minimap_file = inputDir+"/{sample}_chunk{chunkID}.maf.gz",
		soft_clust_file = softClustDir+"/{sample}_chunk{chunkID}_clusters.RData"
	output: outputDir+"/{sample}_chunk{chunkID}.maf"
	log: "log/add_soft_clust_{sample}_chunk{chunkID}.log"
	script: "utils/addSoftProbs.R"

rule append_clusters_to_pb_read_names:
	input:
		minimap_file=expand(outputDir+"/{{sample}}_chunk{chunkID}.maf", chunkID=chunkID),
		pb="export_fasta_from_bam/"+config["pb_fasta"]+".fasta"
	output: outputDir + "/{sample}_with_cluster.fasta"
	run:
		# mapping PacBio readNames to their clusters and soft probs
		pb_name_to_clust={}
		for infile in input["minimap_file"]:
			with open(infile) as clustpb:
				for line in clustpb:
					sp = line.strip().split()
					if sp[0] != "PBreadNames":
						pb_name_to_clust[sp[0]]=(sp[14], sp[15])
		
		# writing PacBio reads clusters in the PacBio fasta file
		with open(input["pb"]) as f:
			with open(output[0], 'w') as out:
				for line in f:
					if line[0]==">":
						sp = line.split()[0].split("_")		
						if len(sp) > 3:
							pb_name = "_".join(sp[:-4])[1:]		# remove the '>' char and the extra info (4 last elements in the '_' splitted list) added to the pb read name
						if len(sp) > 3 and pb_name in pb_name_to_clust:
							pbclust = pb_name_to_clust[pb_name]
							print(line.split()[0] + "\t" + pbclust[0] + "\t" + pbclust[1], file=out)
						else:
							print(line.split()[0] + "\tNone\t1", file=out)
					else:
						print("sequence" + line.strip(), file=out)

rule split_fasta_file_by_cluster:
	input: outputDir + "/{sample}_with_cluster.fasta"
	output: temp(expand(outputDir + "/splitted-per-cluster/{{sample}}_cluster{clust}_temp.fasta", clust=clusters)),
	log: "log/split_fasta_file_by_cluster_{sample}.log"
	shell: 
		'''
		(time set +o pipefail && 
		awk 'BEGIN{{RS=\">\"}} NR>1 {{gsub(\"\\n\", \"\\t\"); print \">\"$0}}' {input} | 	# put every read(name + sequence) in one single line
		awk '$2!=\"None\" {{print>\"{outputDir}/splitted-per-cluster/{wildcards.sample}_cluster\"$2\"_temp.fasta\"}}' && 
		touch {output}) > {log} 2>&1
		'''

# This rule converts the temporary files (which have each read (name+seq) in one line) to the right fasta format files
rule correct_fasta_files:
	input: outputDir + "/splitted-per-cluster/{sample}_cluster{clust}_temp.fasta"
	output: outputDir + "/splitted-per-cluster/{sample}_cluster{clust}.fasta"
	log: "log/correct_fasta_files_{sample}_{clust}.log"
	shell: "(time awk \'{{gsub(\"\\tsequence\", \"\\n\")}}; {{print $0}}\' {input} > {output}) > {log} 2>&1"


rule merge_and_correct_dir_paired_clust_fasta:
	input:
		lambda wc: expand(outputDir + "/splitted-per-cluster/" + wc.sample + "_cluster{pair}.fasta", pair = [wc.clust, clustpair[wc.clust]]),
	output:
		outputDir+"/splitted-per-cluster/{sample}_cluster{clust}_with_RCpair.fasta"
	run:
		os.system("cat " + input[0] + " > " + output[0])
		revcompfasta(input[1], output[0])


rule output_header:
	input: expand(outputDir+"/{sample}_chunk{chunkID}.maf", sample=sample, chunkID=chunkID)
	output: outputDir + "/header.txt"
	log: "log/output_header.log"
	shell: "(time set +o pipefail && head -1 {input[0]} > {output})  > {log} 2>&1"

rule remove_slash_in_lib_names:
	input: outputDir+"/{sample}_chunk{chunkID}.maf",
	output: outputDir+"/{sample}_chunk{chunkID}.new.maf"
	log: "log/remove_slash_in_lib_names_{sample}_chunk{chunkID}.log"
	shell: "(time tail -n +2 {input} | awk '{{sub(/.{{1}}/, \"\", $3)}}1' > {output}) > {log} 2>&1"

rule split_chunk_by_lib:
	input:
		minimap_file = outputDir+"/{sample}_chunk{chunkID}.new.maf"
	output: temp([outputDir+"/splitted-per-lib/{sample}_chunk{chunkID}_" + x for x in expand("{lib}.maf", lib=libs)]),
	log: "log/split_chunk_by_lib_{sample}_chunk{chunkID}"
	shell: "(time awk '{{print>\"{outputDir}/splitted-per-lib/{wildcards.sample}_chunk{wildcards.chunkID}_\"$3\".maf\"}}' {input.minimap_file} && touch {output}) > {log} 2>&1"

rule merge_files_with_same_lib:
	input: [outputDir + x + "{lib}.maf" for x in expand("/splitted-per-lib/{sample}_chunk{chunkID}_", sample=sample, chunkID=chunkID)]
	output: temp(outputDir+"/splitted-per-lib/aln_{lib}.maf")
	log: "log/merge_files_with_same_lib_{lib}.log"
	shell: "(time cat {input} > {output}) > {log} 2>&1"

rule sort_lib_alignments:
	input:
		header = outputDir + "/header.txt",
		#splittedChunks = [outputDir + x + "{lib}.maf" for x in expand("/splitted-per-lib/{sample}_chunk{chunkID}_", sample=sample, chunkID=chunkID)],
		libAlignment = outputDir+"/splitted-per-lib/aln_{lib}.maf"
	output: outputDir+"/splitted-per-lib/aln_{lib}_sorted.maf"
	log: "log/sort_lib_alignments_{lib}.log"
	shell: "(time cat {input.header} > {output} && sort -k1,1 {input.libAlignment} >> {output}) > {log} 2>&1"

rule cluster_SS_reads:
	input:
		aln_lib = outputDir+"/splitted-per-lib/aln_{lib}_sorted.maf",
		clust_to_chrom_mapping = outputDir + "/cluster_name_mapping.data",
		cluster_pairs = outputDir + "/clust_partners.txt"
	output: outputDir+"/splitted-per-lib/clust_{lib}.data"
	log: "log/cluster_SS_reads_{lib}.log"
	script: "utils/cluster_SS_reads.snakemake.R"

rule split_SS_read_names_by_clust:
	input: outputDir+"/splitted-per-lib/clust_{lib}.data"
	output: expand(outputDir+"/splitted-per-lib/clust_splitted_{{lib}}_{cluster}.data", cluster=clusters)
	log: "log/split_SS_read_names_by_clust_{lib}.log"
	shell: "(time awk -F, '{{if ($2 != \"\")print $1> \"{outputDir}/splitted-per-lib/clust_splitted_\" $3 \"_\" $2 \".data\"}}' {input}) > {log} 2>&1"

rule extract_fastq_per_lib_per_clust:
	input:
		SS_read_names = outputDir+"/splitted-per-lib/clust_splitted_{lib}_{cluster}.data",
		SS_fastq = SSfastqDir + "/{lib}_L{L}_R1_001.fastq.gz"
	output: SSfastqDir + "/{lib}_L{L}_R1_001_{cluster}.fastq"	
	log: "log/extract_fastq_per_lib_per_clust.log_{lib}_{L}_{cluster}"
	shell: "(time {EXTRACTFASTQ} {input} > {output}) > {log} 2>&1"

rule merge_fastq_files_with_same_clust:
	input: expand(SSfastqDir + "/{fastqprefix}_R1_001_{{cluster}}.fastq", fastqprefix=[libs[i] + "_L" + L[i] for i in range(len(libs))])
	output: SSfastqDir + "/all_SS_libs_cluster{cluster}.fastq"
	log: "log/merge_fastq_files_with_same_clust_{cluster}.log"
	shell: "(time cat {input} > {output}) > {log} 2>&1"


#################################################
####     De Bruijn graph building MIKKO      ####
#################################################

rule format_input_files:
	output:
		temp("filelist")
	shell:
		"printf '%s\\n' {SHORTREADS}/*.fastq.gz > {output}"

rule run_bcalm2:
	input:
		"filelist"
	output:
		temp("filelist_k{k}_a{a}.unitigs.fa")
	log: "log/run_bcalm2_k{k}_a{a}.log"
	shadow: "shallow"
	threads: 8
	shell:
		"(time {BCALMPATH} -in {input} -out filelist_k{wildcards.k}_a{wildcards.a} -kmer-size {wildcards.k} -abundance-min {wildcards.a} -nb-cores {threads}) > {log} 2>&1"

rule filter_unitig_coverage:
	input:
		"filelist_k{k}_a{a}.unitigs.fa"
	output:
		temp("tmp/filelist_k{k}_a{a}_u{u}.unitigs.fa")
	log: "log/filter_unitig_coverage_k{k}_a{a}_u{u}.log"
	shell:
		"(time python {SCRIPTPATH}/filter_bcalm_by_frequency.py {input} {wildcards.u} {output}) > {log} 2>&1"

rule convert_graph:
	input:
		"tmp/filelist_k{k}_a{a}_u{u}.unitigs.fa"
	output:
		temp("tmp/graph_k{k}_a{a}_u{u}_tipped.gfa")
	log: "log/convert_graph_k{k}_a{a}_u{u}.log"
	shell:
		"(time {CONVERTTOGFAPATH} {input} {output} {wildcards.k}) > {log} 2>&1"

rule untip_graph:
	input:
		"tmp/graph_k{k}_a{a}_u{u}_tipped.gfa"
	output:
		temp("tmp/graph_k{k}_a{a}_u{u}_untip.gfa")
	log: "log/untip_graph_k{k}_a{a}_u{u}.log"
	shell:
		"(time {ALIGNERBINPATH}/UntipRelative 1000 100 0.1 < {input} > {output}) > {log} 2>&1"

rule extract_biggest_component:
	input:
		"tmp/graph_k{k}_a{a}_u{u}_untip.gfa"
	output:
		"tmp/graph_k{k}_a{a}_u{u}_component.gfa"
	log: "log/extract_biggest_component_k{k}_a{a}_u{u}.log"
	shell:
		"(time python {SCRIPTPATH}/extract_gfa_biggest_component.py {input} > {output}) > {log} 2>&1"

rule get_contigs:
	input:
		"tmp/graph_k{k}_a{a}_u{u}_component.gfa"
	output:
		"contigs_k{k}_a{a}_u{u}.fa"
	log: "log/get_contigs_k{k}_a{a}_u{u}.log"
	shell:
		"(time grep S {input} | awk '{{print \">\"$2\"\\n\"$3}}' > {output}) > {log} 2>&1"

rule remake_bcalm_from_contigs:
	input:
		"contigs_k{k}_a{a}_u{u}.fa"
	output:
		"contigs_k{k}_a{a}_u{u}.unitigs.fa"
	log: "log/remake_bcalm_from_contigs_k{k}_a{a}_u{u}.log"
	shadow: "shallow"
	threads: 8
	shell:
		"(time {BCALMPATH} -in {input} -out contigs_k{wildcards.k}_a{wildcards.a}_u{wildcards.u} -abundance-min 1 -kmer-size {wildcards.k} -nb-cores {threads}) > {log} 2>&1"

rule final_graph:
	input:
		"contigs_k{k}_a{a}_u{u}.unitigs.fa"
	output:
		"tmp/graph_k{k}_a{a}_u{u}.gfa"
	log: "log/final_graph_k{k}_a{a}_u{u}.log"
	shell:
		"(time {CONVERTTOGFAPATH} {input} {output} {wildcards.k}) > {log} 2>&1"

rule convert_gfa_to_vg:
	input: "tmp/graph_k{k}_a{a}_u{u}.gfa"
	output: "tmp/graph_k{k}_a{a}_u{u}.vg"
	log: "log/convert_gfa_to_vg_k{k}_a{a}_u{u}.log"
	log: "log/convert_gfa_to_vg_k{k}_a{a}_u{u}.log"
	shell: "(time {VG} view -v -F {input} > {output}) > {log} 2>&1"

rule align_SS_reads_to_DBG:
	input:
		SS_fastq = SSfastqDir + "/all_SS_libs_cluster{cluster}.fastq",
		graph = "contigs_k{k}_a{a}_u{u}.unitigs.fa"
	output: "graph_alignment/aln_cluster{cluster}_params_k{k}_a{a}_u{u}.data"
	threads: 8
	log: "log/align_SS_reads_to_DBG_cluster{cluster}_params_k{k}_a{a}_u{u}.log"
	shell: "(time {BGREAT} -t {threads} -k {wildcards.k} -u {input.SS_fastq} -g {input.graph} -q -a 31 -f {output}) > {log} 2>&1"


######################################
#finding heterozygous unitigs
######################################

## computing the ground truth het unitifs:
## mapping unitigs to the reference genome

rule index_ref:
	input: config["reference"]
	output:
		config["reference"] + ".amb",
		config["reference"] + ".ann",
		config["reference"] + ".bwt",
		config["reference"] + ".pac",
		config["reference"] + ".sa"
	log: "log/bwa_index.log"
	shell: "(time bwa index {input}) > {log} 2>&1"

rule bwa_map_unitigs_to_ref:
	input:
		ref=config["reference"],
		unitigs="contigs_k{k}_a{a}_u{u}.unitigs.fa"
	output: "mapped_contigs_k{k}_a{a}_u{u}.unitigs.bam"
	log: "log/bwa_map_unitigs_to_ref_k{k}_a{a}_u{u}.log"
	shell:
		"(time bwa mem -t 32 {input} | samtools view -Sb - > {output}) > {log} 2>&1"

rule sort_bam:
	input: "mapped_contigs_k{k}_a{a}_u{u}.unitigs.bam",
	output: "mapped_contigs_k{k}_a{a}_u{u}.unitigs.sorted.bam"
	log: "log/sort_bam_k{k}_a{a}_u{u}.log"
	shell: "(time samtools sort -o {output} {input}) > {log} 2>&1"

#rule index_sorted_bam:
#	input: "mapped_contigs_k{k}_a{a}_u{u}.unitigs.sorted.bam"
#	output : "mapped_contigs_k{k}_a{a}_u{u}.unitigs.sorted.bam.bai"
#	log: "log/index_bam_k{k}_a{a}_u{u}.log"
#	shell: "(time set +o pipefail && samtools index {input}) > {log} 2>&1"

rule intersect_bam_vcf:
	input: 
		bam="mapped_contigs_k{k}_a{a}_u{u}.unitigs.sorted.bam",
		vcf=[config["vcf_prefix"] + c + ".vcf" for c in chroms]
	output: "mapped_contigs_k{k}_a{a}_u{u}.heterozygous.unitigs.sorted.bam"
	log: "log/intersect_bam_vcf_k{k}_a{a}_u{u}.log"
	shell: "(time bedtools intersect -a {input.bam} -b {input.vcf} > {output}) > {log} 2>&1"

rule intersect_bedvcf_het_unitigs:
	input:
		bam="mapped_contigs_k{k}_a{a}_u{u}.heterozygous.unitigs.sorted.bam",
		bedvcf="NA12878_hg38_GIAB_het_pos.bed"
	output: "mapped_contigs_k{k}_a{a}_u{u}.heterozygous.unitigs.sorted.bed"
	log: "log/intersect_bedvcf_het_unitigs_k{k}_a{a}_u{u}.log"
	shell: "(time bedtools intersect -a {input.bedvcf} -b {input.bam} > {output}) > {log} 2>&1"


rule export_het_unitigs_fasta:
	input: "mapped_contigs_k{k}_a{a}_u{u}.heterozygous.unitigs.sorted.bam"
	output: "mapped_contigs_k{k}_a{a}_u{u}.heterozygous.unitigs.sorted.fa"
	log: "log/export_het_unitigs_fasta_k{k}_a{a}_u{u}.log"
	shell: "(time perl {exportFasta} {input} > {output}) > {log} 2>&1"

## debug
#rule export_revcomp_free_hetunitigs_fasta:
#	input: "mapped_contigs_k{k}_a{a}_u{u}.heterozygous.unitigs.sorted.fa"
#	output: "mapped_contigs_k{k}_a{a}_u{u}.heterozygous.unitigs.sorted.revcompfree.fa"
#	log: "log/export_revcomp_free_hetunitigs_fasta.log"
#	shell: "(time set +o pipefail && python {getRevcompFree} {input} > {output}) > {log} 2>&1"


## bubble detection
rule bubble_detection:
	input: "tmp/graph_k{k}_a{a}_u{u}.gfa"
	output: "tmp/bubbles_k{k}_a{a}_u{u}.fa"
	log: "log/bubble_detection_k{k}_a{a}_u{u}.log"
	shell: "(time python {computeSimpleBubble} {input} > {output}) > {log} 2>&1"

rule output_snv_bubbles:
	input: "tmp/bubbles_k{k}_a{a}_u{u}.fa"
	output: "tmp/snv_bubbles_k{k}_a{a}_u{u}.fa"
	log: "log/output_snv_bubbles_k{k}_a{a}_u{u}.log"
	shell: "(time python {getSNVbubbles} {input} > {output}) > {log} 2>&1"

rule bwa_map_snv_bubble_unitigs_to_ref:
	input:
		ref=config["reference"],
		unitigs="tmp/snv_bubbles_k{k}_a{a}_u{u}.fa"
	output: "mapped_contigs_k{k}_a{a}_u{u}.snv.bubbles.unitigs.bam"
	log: "log/bwa_map_snv_bubble_unitigs_to_ref_k{k}_a{a}_u{u}.log"
	shell:
		"(time bwa mem -t 32 {input} | samtools view -Sb - > {output}) > {log} 2>&1"

rule sort_bam_snv_bubbles:
	input: "mapped_contigs_k{k}_a{a}_u{u}.snv.bubbles.unitigs.bam"
	output: "mapped_contigs_k{k}_a{a}_u{u}.snv.bubbles.unitigs.sorted.bam"
	log: "log/sort_bam_snv_bubbles_k{k}_a{a}_u{u}.log"
	shell: "(time samtools sort -o {output} {input}) > {log} 2>&1"

rule intersect_bam_vcf_snv_bubbles:
	input: 
		bam="mapped_contigs_k{k}_a{a}_u{u}.snv.bubbles.unitigs.sorted.bam",
		vcf=[config["vcf_prefix"] + c + ".vcf" for c in chroms]
	output: "mapped_contigs_k{k}_a{a}_u{u}.heterozygous.snv.bubbles.unitigs.sorted.bam"
	log: "log/intersect_bam_vcf_snv_bubbles_k{k}_a{a}_u{u}.log"
	shell: "(time bedtools intersect -a {input.bam} -b {input.vcf} > {output}) > {log} 2>&1"

rule vcf_to_bed:
	input:[config["vcf_prefix"] + c + ".vcf" for c in chroms]
	output:"NA12878_hg38_GIAB_het_pos.bed"
	log: "log/vcf_to_bed.log"
	shell:"(time cat {input} | grep \"^[^#]\" | awk \'{{print $1\"\t\"$2-1\"\t\"$2}}\' > {output}) > {log} 2>&1"

rule intersect_bedvcf_snv_bubbles:
	input:
		bam="mapped_contigs_k{k}_a{a}_u{u}.heterozygous.snv.bubbles.unitigs.sorted.bam",
		bedvcf="NA12878_hg38_GIAB_het_pos.bed"
	output: "mapped_contigs_k{k}_a{a}_u{u}.heterozygous.snv.bubbles.unitigs.sorted.bed"
	log: "log/intersect_bedvcf_snv_bubbles_k{k}_a{a}_u{u}.log"
	shell: "(time bedtools intersect -a {input.bedvcf} -b {input.bam} > {output}) > {log} 2>&1"


## debug
#rule export_revcomp_free_snvbubbles_fasta:
#	input: "tmp/snv_bubbles_k{k}_a{a}_u{u}.fa"
#	output: "tmp/snv_bubbles_k{k}_a{a}_u{u}.revcompfree.fa"
#	log: "log/export_revcomp_free_snvbubbles_fasta.log"
#	shell: "(time python {computeSimpleBubble} {input} > {output}) > {log} 2>&1"


######################################
####   	  k-mer counting	  ####
######################################

rule get_contig_kmer_coverage:
	input: "tmp/graph_k{k}_a{a}_u{u}_component.gfa"
	output: "tmp/graph_k{k}_a{a}_u{u}_component_km.txt"
	log: "log/get_contig_kmer_coverage_k{k}_a{a}_u{u}.log"
	shell: "(time awk \'$1!=\"S\" {{gsub(\"km:f:\", \"\"); print $6}}\' {input} > {output}) > {log} 2>&1"

rule get_contig_kmer_coverage_histogram:
	input: "tmp/graph_k{k}_a{a}_u{u}_component_km.txt"
	output: "tmp/graph_k{k}_a{a}_u{u}_component_km_hist.pdf"
	params:
		trim=0.05,
		breaks=200
	log: "get_contig_kmer_coverage_histogram_k{k}_a{a}_u{u}.log"
	script: "utils/contig_kmer_coverage_histogram.snakemake.R"

rule jellyfish_bloom_count_kmers:
	input: "filelist"
	output:	"short_reads_{k}mer_count.bc"
	log: "log/bloom_count_kmers_k{k}.log"
	threads: 32
	shell: 
		'''
		(time set +o pipefail zcat {input} | jellyfish bc -m {wildcards.k} -s 200G -t {threads} -C -o {output}) > {log} 2>&1
		'''

# This rule does not work properly
rule jellyfish_count_kmers:
	input: 
		reads="filelist",
		bc="short_reads_{k}mer_count.bc"
	output:	"short_reads_{k}mer_count.jf"
	log: "log/count_kmers_k{k}.log"
	threads: 32
	shell: 
		'''
		(time set +o pipefail zcat {input.reads} | jellyfish count -m {wildcards.k} -s 3G -t {threads} -C --bc {input.bc} -o {output}) > {log} 2>&1
		'''

rule convert_jf_to_fa:
	input: "short_reads_{k}mer_count.jf"
	output: "short_reads_{k}mer_count.fa"
	log: "log/convert_jf_to_fa_k{k}.log"
	shell: "(time jellyfish dump {input} > {output}) > {log} 2>&1"


######################################
rule output_unique_covered_nodes:
	input: "graph_alignment/aln_cluster{cluster}_params_k{k}_a{a}_u{u}.data"
	output: "graph_alignment/nodes_cluster{cluster}_params_k{k}_a{a}_u{u}.data"
	run:
		lines = open(input[0]).read()
		l = list(set(lines.replace(";", " ").split()))
		open(output[0], "w").write("\n".join(l))

rule find_inter_chr_intersection:
	input:
		nodes = expand("graph_alignment/nodes_cluster{cluster}_params_k{k}_a{a}_u{u}.data", cluster=clusters, k=config["k"], a = config["kmer_abundance"], u = config["unitig_abundance"]),
		cluster_pairs = outputDir + "/clust_partners.txt",
		node_lens = nodelens
	output:	"graph_alignment/graph_nodes_inter_chr_intersection.RData"
	log: "log/find_inter_chr_intersection.log"
	script: "utils/find_inter_chr_intersection.snakemake.R"


#######################################
#####	     Canu assembly	  #####
#######################################

rule estimate_cluster_sizes:
	input:
		chromsize=config["chrom_sizes"],		
		clustersfasta=expand(outputDir+"/splitted-per-cluster/{sample}_cluster{clust}_with_RCpair.fasta", sample=sample, clust=clustpair.keys())
	output: config["cluster_sizes"]
	shell:
		'''
		paste {input.chromsize} <(ls -lh {outputDir}/splitted-per-cluster/*_with_RCpair.fasta | awk '{{gsub(\"G\", \"\", $5); gsub(\"{outputDir}/splitted-per-cluster/{sample}_cluster", \"\"); gsub(\"_with_RCpair.fasta\", \"\"); print}}' | sort -n -k5,5 | cut -d\' \' -f9) > {output}
		'''

rule correct_reads:
	input:
		fasta=outputDir+"/splitted-per-cluster/{sample}_cluster{clust}_with_RCpair.fasta",
		clustersizes=config["cluster_sizes"]
	output:
		outputDir+"/splitted-per-cluster/Canu-assembly_{sample}_cluster{clust}_with_RCpair/PacBio.correctedReads.fasta.gz"
	log:
		"log/correct_reads_{sample}_cluster{clust}_with_RCpair.log"
	params:
		genomeSize = lambda wc: getclustersize(wc.clust, clustersizes)
	threads: 32

	shell:
		"""
		time(canu -correct \
		-p PacBio \
		-d {outputDir}/splitted-per-cluster/Canu-assembly_{wildcards.sample}_cluster{wildcards.clust}_with_RCpair \
		genomeSize={params.genomeSize} \
		-minThreads=1 \
		-maxThreads={threads} \
		-pacbio-raw {input.fasta}) 2> {log}
		"""

rule trim_reads:
	input:
		corrected=outputDir+"/splitted-per-cluster/Canu-assembly_{sample}_cluster{clust}_with_RCpair/PacBio.correctedReads.fasta.gz",
		clustersizes=config["cluster_sizes"]
		
	output:
		outputDir+"/splitted-per-cluster/Canu-assembly_{sample}_cluster{clust}_with_RCpair/PacBio.trimmedReads.fasta.gz"
	log:
		"log/trim_reads_{sample}_cluster{clust}_with_RCpair.log"
	params:
		genomeSize = lambda wc: getclustersize(wc.clust, clustersizes)
	threads: 32

	shell:
		"""
		time(canu -trim \
		-p PacBio \
		-d {outputDir}/splitted-per-cluster/Canu-assembly_{wildcards.sample}_cluster{wildcards.clust}_with_RCpair \
		genomeSize={params.genomeSize} \
		-minThreads=1 \
		-maxThreads={threads} \
		-pacbio-corrected {input.corrected}) 2> {log}
		"""

rule assembly:
	input:
		trimmed=outputDir+"/splitted-per-cluster/Canu-assembly_{sample}_cluster{clust}_with_RCpair/PacBio.trimmedReads.fasta.gz",
		clustersizes=config["cluster_sizes"]

	output:
		outputDir+"/splitted-per-cluster/Canu-assembly_{sample}_cluster{clust}_with_RCpair/PacBio_errorRate{error}_overlap{overlap}.contigs.fasta"

	log:
		"log/assembly_{sample}_cluster{clust}_with_RCpair.log"
	params:
		genomeSize = lambda wc: getclustersize(wc.clust, clustersizes),
		#errorRate = 0.1,
		#overlapLen = 200
	threads: 32

	shell:
		"""
		time(canu -assemble \
		-p PacBio_errorRate{wildcards.error}_overlap{wildcards.overlap} \
		-d {outputDir}/splitted-per-cluster/Canu-assembly_{wildcards.sample}_cluster{wildcards.clust}_with_RCpair \
		genomeSize={params.genomeSize} \
		-minThreads=1 \
		-maxThreads={threads} \
		correctedErrorRate={wildcards.error} \
		minOverlapLength={wildcards.overlap} \
		-pacbio-corrected {input.trimmed}) 2> {log}
		"""


